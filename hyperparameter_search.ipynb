{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "import gym_mtsim\n",
        "sys.path.append(\"C:/Users/WilliamFetzner/Documents/Trading/\")\n",
        "from gym_mtsim_forked.gym_mtsim.data import FOREX_DATA_PATH, FOREX_DATA_PATH_1HR, FOREX_DATA_PATH_15MIN, FOREX_DATA_PATH_5MIN\n",
        "from gym_mtsim import OrderType, Timeframe, MtEnv, MtSimulator\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, STATUS_FAIL\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "import time\n",
        "import torch\n",
        "import pickle\n",
        "import fx_rl\n",
        "from datetime import datetime, timedelta\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Latest Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "current_time = pd.to_datetime(now) + timedelta(hours=7)\n",
        "current_time = current_time.replace(tzinfo=pytz.UTC)\n",
        "# date_15min = fx_rl.bars_back(current_time, 'M15')\n",
        "# date_5min = fx_rl.bars_back(current_time, 'M5')\n",
        "date_1hr = fx_rl.bars_back(current_time, 'H1', total_bars=50_000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim = MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=10000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=False,\n",
        ")\n",
        "sim.download_data(\n",
        "    symbols=['EURUSD', 'AUDCHF', 'NZDCHF', 'GBPNZD', 'USDCAD'],\n",
        "    time_range=(\n",
        "        date_1hr,\n",
        "        current_time\n",
        "    ),\n",
        "    timeframe=Timeframe.H1\n",
        ")\n",
        "sim.save_symbols(FOREX_DATA_PATH)\n",
        "# sim.save_symbols(f'symbols_forex_15min_{date_1hr.date()}_{current_time.date()}.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'symbols_forex_5min_2023-01-09_2024-05-08', 'rb') as f:\n",
        "    symbols_5min = pickle.load(f)\n",
        "# convert symbols_5min to a pd.dataframe\n",
        "symbols_5min[1]['EURUSD'].index = pd.to_datetime(symbols_5min[1]['EURUSD'].index)\n",
        "max_date_5min = symbols_5min[1]['EURUSD'].index.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'symbols_forex_15min_2020-05-11_2024-05-08', 'rb') as f:\n",
        "    symbols_15min = pickle.load(f)\n",
        "# convert symbols_15min to a pd.dataframe\n",
        "symbols_15min[1]['EURUSD'].index = pd.to_datetime(symbols_15min[1]['EURUSD'].index)\n",
        "max_date_15min = symbols_15min[1]['EURUSD'].index.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(FOREX_DATA_PATH, 'rb') as f:\n",
        "    symbols_1hr = pickle.load(f)\n",
        "# convert symbols_1hr to a pd.dataframe\n",
        "symbols_1hr[1]['EURUSD'].index = pd.to_datetime(symbols_1hr[1]['EURUSD'].index)\n",
        "max_date_1hr = symbols_1hr[1]['EURUSD'].index.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # unpack the pickle file and load the data that is in symbols_forex.pkl\n",
        "# with open(FOREX_DATA_PATH, 'rb') as f:\n",
        "#     symbols = pickle.load(f)\n",
        "# # convert symbols to a pd.dataframe\n",
        "# # symbols[1]['EURUSD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slices_5min = fx_rl.slices_finder(symbols_5min[1]['EURUSD'], max_date_5min, testing_needed=True)\n",
        "slices_15min = fx_rl.slices_finder(symbols_15min[1]['EURUSD'], max_date_15min, testing_needed=True)\n",
        "slices_1hr = fx_rl.slices_finder(symbols_1hr[1]['EURUSD'], max_date_1hr, testing_needed=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class MyMtEnv(gym_mtsim.MtEnv):\n",
        "#     # _get_modified_volume = fx_rl.my_get_modified_volume\n",
        "#     _get_prices = fx_rl.my_get_prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_train_1hr = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH_1HR\n",
        ")\n",
        "\n",
        "env_train_1hr = gym_mtsim.MtEnv(\n",
        "    original_simulator=sim_train_1hr,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=slices_1hr[0],\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_train_15min = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH_15MIN\n",
        ")\n",
        "\n",
        "env_train_15min = gym_mtsim.MtEnv(\n",
        "    original_simulator=sim_train_15min,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=slices_15min[0],\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_train_5min = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH_5MIN\n",
        ")\n",
        "\n",
        "env_train_5min = gym_mtsim.MtEnv(\n",
        "    original_simulator=sim_train_5min,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=slices_5min[0],\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_validation_1hr = MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_validation_1hr = gym_mtsim.MtEnv(\n",
        "    original_simulator=sim_validation_1hr,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=slices_1hr[1],\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_validation_15min = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH_15MIN\n",
        ")\n",
        "\n",
        "env_validation_15min = gym_mtsim.MtEnv(\n",
        "    original_simulator=sim_validation_15min,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=slices_15min[1],\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_validation_5min = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH_5MIN\n",
        ")\n",
        "\n",
        "env_validation_5min = gym_mtsim.MtEnv(\n",
        "    original_simulator=sim_validation_5min,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=slices_5min[1],\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sim_testing = gym_mtsim.MtSimulator(\n",
        "#     unit='USD',\n",
        "#     balance=200000.,\n",
        "#     leverage=100.,\n",
        "#     stop_out_level=0.2,\n",
        "#     hedge=True,\n",
        "#     symbols_filename=FOREX_DATA_PATH\n",
        "# )\n",
        "\n",
        "# env_testing = MyMtEnv(\n",
        "#     original_simulator=sim_testing,\n",
        "#     trading_symbols=['EURUSD'],\n",
        "#     window_size = 10,\n",
        "#     time_points=slices_1hr[2],\n",
        "#     hold_threshold=0.5,\n",
        "#     close_threshold=0.5,\n",
        "#     fee=lambda symbol: {\n",
        "#         # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#         'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#         # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#     }[symbol],\n",
        "#     symbol_max_orders=2,\n",
        "#     multiprocessing_processes=2\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(reward_over_episodes, printing_name):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg_rewards = np.mean(reward_over_episodes)\n",
        "    min_rewards = np.min(reward_over_episodes)\n",
        "    max_rewards = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. {printing_name}          : {min_rewards:>10.3f}')\n",
        "    print (f'Avg. {printing_name}          : {avg_rewards:>10.3f}')\n",
        "    print (f'Max. {printing_name}          : {max_rewards:>10.3f}')\n",
        "\n",
        "    return min_rewards, avg_rewards, max_rewards\n",
        "\n",
        "def my_profit_calculation(env_orders, stop_loss):\n",
        "        # env_orders = env_testing.render()['orders']\n",
        "        # stop_loss = 0.001\n",
        "        mean_value = env_orders['Volume'].mean()\n",
        "\n",
        "        # # Normalize the column to have a mean of 1\n",
        "        env_orders.loc[:, 'Volume'] = round((env_orders['Volume'] / mean_value), 2)\n",
        "\n",
        "        # add a column for when the difference between the Entry Price and the Exit Price is greater than stop_loss\n",
        "        env_orders.loc[:, 'stoploss_hit'] = np.where((env_orders['Type'].str.strip() == 'Buy') &\n",
        "                                                        ((env_orders['Entry Price'] - env_orders['Exit Price']) > stop_loss),\n",
        "                                                        1, np.where((env_orders['Type'].str.strip() == 'Sell') &\n",
        "                                                                        ((env_orders['Exit Price'] - env_orders['Entry Price']) > stop_loss),\n",
        "                                                                        1, 0))\n",
        "        env_orders.loc[:, 'Exit Price'] = np.where((env_orders['Type'].str.strip() == 'Buy') & (env_orders['stoploss_hit'] == 1),\n",
        "                                                        env_orders['Entry Price'] - stop_loss,\n",
        "                                                        np.where((env_orders['Type'].str.strip() == 'Sell') & (env_orders['stoploss_hit'] == 1),\n",
        "                                                                env_orders['Entry Price'] + stop_loss, env_orders['Exit Price']))\n",
        "        env_orders.loc[:, 'Profit'] = np.where((env_orders['Type'].str.strip() == 'Buy'),\n",
        "                                                        ((env_orders['Exit Price'] - (env_orders['Fee']/2)) - \n",
        "                                                        (env_orders['Entry Price'] + (env_orders['Fee']/2)))\n",
        "                                                                * 100_000 * env_orders['Volume'], \n",
        "                                                        np.where((env_orders['Type'].str.strip() == 'Sell'),\n",
        "                                                                ((env_orders['Entry Price'] - (env_orders['Fee']/2)) - \n",
        "                                                                (env_orders['Exit Price'] + (env_orders['Fee']/2)))\n",
        "                                                                * 100_000 * env_orders['Volume'], np.nan))\n",
        "        total_reward = env_orders.loc[:, 'Profit'].sum()\n",
        "        # Calculate Gross Profit\n",
        "        gross_profit = env_orders.loc[env_orders['Profit'] > 0, 'Profit'].sum()\n",
        "\n",
        "        # Calculate Gross Loss\n",
        "        gross_loss = env_orders.loc[env_orders['Profit'] < 0, 'Profit'].abs().sum()\n",
        "\n",
        "        # Calculate Profit Factor\n",
        "        profit_factor = gross_profit / gross_loss if gross_loss != 0 else 0\n",
        "\n",
        "        profit_factor = profit_factor - 1\n",
        "\n",
        "        return profit_factor, total_reward\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "    \n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "space = {\n",
        "    # 'learning_rate': hp.loguniform('learning_rate', -5, -2), # Learning rate\n",
        "    'learning_rate': hp.uniform('learning_rate', 0, 0.04), # Learning rate\n",
        "    'gamma': hp.uniform('gamma', 0.925, 0.975), # Discount factor\n",
        "    # 'ent_coef': hp.loguniform('ent_coef', -5, 0) # Entropy coefficient\n",
        "    'ent_coef': hp.uniform('ent_coef', 0, 0.05), # Entropy coefficient\n",
        "    # 'learning_timesteps': hp.choice('learning_timesteps', [25, 50, 100, 250, 500]),\n",
        "    # 'timeframe': hp.choice('timeframe', ['5min', '15min', '1hr'])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING + TEST\n",
        "def train_val_model(model, model_policy, env_tr, env_val, seed, steps_str, lr, gamma_param, entropy, total_learning_timesteps=10_000):\n",
        "    \"\"\"\n",
        "    Trains and validates a model using the Proximal Policy Optimization (PPO) algorithm.\n",
        "\n",
        "    Args:\n",
        "        model (object): The model to be trained.\n",
        "        model_policy (object): The policy used by the model.\n",
        "        env_tr (object): The training environment.\n",
        "        env_val (object): The validation environment.\n",
        "        seed (int): The random seed for reproducibility.\n",
        "        steps_str (str): A string representing the number of steps.\n",
        "        window_size_param (int): The window size parameter.\n",
        "        lr (float): The learning rate.\n",
        "        gamma_param (float): The gamma parameter.\n",
        "        entropy (float): The entropy coefficient.\n",
        "        total_learning_timesteps (int, optional): The total number of learning timesteps. Defaults to 10,000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the reward over validations, orders over validations, and the model dictionary.\n",
        "    \"\"\"\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    #model_dict = {}\n",
        "    # env_tr.window_size = window_size_param\n",
        "    print(f'entropy: {entropy}, learning rate: {lr}, gamma: {gamma_param}')\n",
        "    # eval_callback = EvalCallback(env_tr, log_path='./logs/', eval_freq=1000)\n",
        "    obs_tr, info_tr = env_tr.reset(seed=seed)\n",
        "    model = PPO(model_policy, env_tr, verbose=0, ent_coef=entropy, learning_rate=lr)#, gamma=gamma_param, \n",
        "    # custom callback for 'progress_bar'\n",
        "    model.learn(total_timesteps=total_learning_timesteps)#, callback=ProgressBarCallback(100))\n",
        "\n",
        "    reward_over_validations = []\n",
        "    orders_over_validations = []\n",
        "    profit_over_validations = []\n",
        "\n",
        "    for episode in range(0, 10):\n",
        "        obs_val, info_val = env_val.reset(seed=seed)\n",
        "\n",
        "        total_reward = 0\n",
        "        done_val = False\n",
        "\n",
        "        while not done_val:\n",
        "            action, _states = model.predict(obs_val)\n",
        "            obs_val, reward_val, terminated_val, truncated_val, info_val = env_val.step(action)\n",
        "            done_val = terminated_val or truncated_val\n",
        "\n",
        "            total_reward += reward_val\n",
        "            if done_val:\n",
        "                break\n",
        "        try:\n",
        "            orders_made_in_episode = env_val.render()['orders']\n",
        "            order_len = len(orders_made_in_episode)\n",
        "            total_reward, total_profit = my_profit_calculation(orders_made_in_episode, 0.001)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f'There were not any orders produced by the model. Error: {e}')\n",
        "            order_len = 0\n",
        "            total_profit = 0\n",
        "\n",
        "        # model_dict[f'model_{episode}'] = model\n",
        "        # model.save(f'best_hyperparameters/models_4_26_24/model_{episode}.pkl')\n",
        "\n",
        "        reward_over_validations.append(total_reward) \n",
        "        profit_over_validations.append(total_profit)   \n",
        "        orders_over_validations.append(order_len)  \n",
        "\n",
        "\n",
        "        # if episode % 1 == 0:\n",
        "        avg_reward = np.mean(reward_over_validations)\n",
        "        avg_orders = np.mean(orders_over_validations)\n",
        "        avg_profit = np.mean(profit_over_validations)\n",
        "        print(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}, # of orders: {avg_orders:.3f}, avg Profit: {avg_profit:.3f}')\n",
        "\n",
        "    return reward_over_validations, orders_over_validations, profit_over_validations#, model_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 10\n",
        "\n",
        "# print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n",
        "\n",
        "# learning_timesteps_list_in_K = [25]#, 50, 100]\n",
        "# learning_timesteps_list_in_K = [50, 250, 500]\n",
        "# learning_timesteps_list_in_K = [500, 1000, 3000, 5000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "\n",
        "timesteps_models_dict = {}\n",
        "def objective(params):\n",
        "    learning_timesteps = 100 #params['learning_timesteps']\n",
        "    ent_coef = params['ent_coef']\n",
        "    gamma = params['gamma'] #0.99 #\n",
        "    learning_rate = params['learning_rate']#0.0003#\n",
        "    # timeframe = params['timeframe']\n",
        "    # if timeframe == '5min':\n",
        "    #     env_train = env_train_5min\n",
        "    #     env_validation = env_validation_5min\n",
        "    # elif timeframe == '15min':\n",
        "    #     env_train = env_train_15min\n",
        "    #     env_validation = env_validation_15min\n",
        "    # elif timeframe == '1hr':\n",
        "    #     env_train = env_train_1hr\n",
        "    #     env_validation = env_validation_1hr\n",
        "\n",
        "    if learning_rate > 0.08:\n",
        "        print(f'Learning rate too high: {learning_rate}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    if ent_coef > 0.1:\n",
        "        print(f'Entropy too high: {ent_coef}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "\n",
        "    total_learning_timesteps = learning_timesteps * 1000\n",
        "    step_key = f'{learning_timesteps}K'\n",
        "    policy_dict = PPO.policy_aliases\n",
        "    policy = policy_dict.get('MultiInputPolicy')\n",
        "    class_name = type(PPO).__qualname__\n",
        "    plot_key = f'{class_name}_rewards_'+step_key\n",
        "    try:\n",
        "        rewards, orders, profits = train_val_model(PPO, policy, env_train_1hr, env_validation_1hr, seed, step_key,  \n",
        "                                                    learning_rate, gamma, ent_coef, total_learning_timesteps)\n",
        "    except Exception as e:\n",
        "        if 'Tensor of shape' in str(e):\n",
        "            print(f'''there was an error with the tensor with those parameters: timesteps: {learning_timesteps}, \\n\n",
        "              ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        else:\n",
        "            print(f'''there was an error {e} with those parameters: timesteps: {learning_timesteps}, \\n\n",
        "                ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    # timesteps_models_dict[step_key] = models_dict\n",
        "    min_rewards, avg_rewards, max_rewards, = print_stats(profits, 'Profits')\n",
        "    print_stats(orders, 'Orders')\n",
        "    label = f'Avg. {avg_rewards:>7.2f} : {class_name} - {step_key}'\n",
        "    plot_data[plot_key] = rewards\n",
        "    plot_settings[plot_key] = {'label': label}\n",
        "    params['avg_orders'] = np.mean(orders)\n",
        "    params['rewards'] = np.mean(rewards)       \n",
        "\n",
        "    return {'loss': -avg_rewards, 'status': STATUS_OK, 'eval_time': time.time(), 'parameters': params} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # # check if it is working:\n",
        "# parameters = {\n",
        "#     # 'window_size': 10,\n",
        "#     # 'learning_timesteps': 25,\n",
        "#     'ent_coef': 0.008841807731982131,\n",
        "#     # 'gamma': 0.9484679718228304,\n",
        "#     'learning_rate': 0.021173768344759137\n",
        "# }\n",
        "# objective(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PPO('MultiInputPolicy', env_train, verbose=0, ent_coef=parameters['ent_coef']).learn(total_timesteps=25_000) #, learning_rate=parameters['learning_rate'], gamma=parameters['gamma'], ent_coef=parameters['ent_coef']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### adding in gamma test ####\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=250, # Number of evaluations of the objective function\n",
        "            trials=trials,\n",
        "            trials_save_file=f'gym_mtsim_forked/examples/hyperopt/trials_5_17_search_next_week.pkl')\n",
        "\n",
        "print(\"Best parameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # make a sound when the code is done\n",
        "# import winsound\n",
        "# frequency = 2500  # Set Frequency To 2500 Hertz\n",
        "# duration = 2000  # Set Duration To 1000 ms == 1 second\n",
        "# winsound.Beep(frequency, duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials = pickle.load(open(f'gym_mtsim_forked/examples/hyperopt/trials_5_17_search_next_week.pkl', 'rb'))\n",
        "for trial in trials.results:\n",
        "    trial['iteration'] = 7\n",
        "trials_all_results = trials.results\n",
        "print(len(trials_all_results),\n",
        "trials_all_results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame()\n",
        "new_dict = {}\n",
        "for idx, result in enumerate(trials_all_results):\n",
        "    new_dict['loss'] = result['loss']\n",
        "    new_dict['status'] = result['status']\n",
        "    new_dict['learning_rate'] = result['parameters']['learning_rate']\n",
        "    new_dict['ent_coef'] = result['parameters']['ent_coef']\n",
        "    new_dict['gamma'] = result['parameters']['gamma']\n",
        "    try:\n",
        "        new_dict['orders'] = result['parameters']['avg_orders']\n",
        "        new_dict['profits'] = result['parameters']['profits']\n",
        "    except Exception as e: \n",
        "        new_dict['orders'] = 0\n",
        "        new_dict['profits'] = 0\n",
        "    new_row = pd.DataFrame(new_dict, index=[idx])\n",
        "    results_df = pd.concat([results_df, new_row], axis=0)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df.loc[:, 'loss_binary'] = np.where(results_df['loss'] < 0, 1, 0)\n",
        "results_df_low_entropy = results_df[results_df['ent_coef'] < 0.1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_palette = sns.color_palette([\"red\", \"green\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and status of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='learning_rate', hue='status', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Learning Rate')\n",
        "# create the second subplot with gamma x entropy and status of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='ent_coef', hue='status', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Gamma x Entropy')\n",
        "# create the third subplot with learning rate x entropy and status of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='learning_rate', y='ent_coef', hue='status', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Entropy')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Binary Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='learning_rate', hue='loss_binary', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Learning Rate')\n",
        "# create the second subplot with gamma x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='ent_coef', hue='loss_binary', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Gamma x Entropy')\n",
        "# create the third subplot with learning rate x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='learning_rate', y='ent_coef', hue='loss_binary', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Entropy')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_low_entropy_negative = results_df_low_entropy[results_df_low_entropy['loss'] < 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Binary Loss Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy_negative, x='gamma', y='learning_rate', hue='loss', ax=axs[0])\n",
        "axs[0].set_title('Gamma x Learning Rate')\n",
        "# create the second subplot with gamma x entropy and loss of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy_negative, x='gamma', y='ent_coef', hue='loss', ax=axs[1])\n",
        "axs[1].set_title('Gamma x Entropy')\n",
        "# create the third subplot with learning rate x entropy and loss of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy_negative, x='learning_rate', y='ent_coef', hue='loss', ax=axs[2])\n",
        "axs[2].set_title('Learning Rate x Entropy')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters vs loss plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='loss', hue='loss_binary', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Loss')\n",
        "# create the second subplot with gamma x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='ent_coef', y='loss', hue='loss_binary', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Entropy x Loss')\n",
        "# create the third subplot with learning rate x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='learning_rate', y='loss', hue='loss_binary', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Loss')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize the parameters that cause failures in the objective function\n",
        "\n",
        "# create a graph that has learning rate on the x-axis and ent_coef on the y-axis, \n",
        "# then the color of the points is whether the status is ok or fail, green for ok and red for fail\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df['learning_rate'], results_df['ent_coef'], \n",
        "                     c=results_df['status'].apply(lambda x: 'green' if x == 'ok' else 'red'))\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "# y lim to 0.2\n",
        "plt.ylim(0, 0.1)\n",
        "# x lim to 0.05\n",
        "# plt.xlim(0, 0.05)\n",
        "# plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Hyperparameters vs Next Week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only successes \n",
        "results_df_success = results_df[results_df['status'] == 'ok']\n",
        "results_df_success_negative = results_df_success[results_df_success['loss'] < 0]\n",
        "# sort values from least to greatest loss\n",
        "results_df_success_negative_sorted = results_df_success_negative.sort_values(by='loss', ascending=True)\n",
        "results_df_success_negative_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_train = MyMtEnv(\n",
        "    original_simulator=sim_train,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(symbols[1]['EURUSD'].loc[:(max_friday - pd.DateOffset(days=7)), :].index),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 10\n",
        "\n",
        "# print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_testing(params):\n",
        "    learning_timesteps = 50 #params['learning_timesteps']\n",
        "    ent_coef = params['ent_coef']\n",
        "    gamma = params['gamma'] #0.99 #\n",
        "    learning_rate = params['learning_rate']#0.0003#\n",
        "\n",
        "    total_learning_timesteps = learning_timesteps * 1000\n",
        "    step_key = f'{learning_timesteps}K'\n",
        "    policy_dict = PPO.policy_aliases\n",
        "    policy = policy_dict.get('MultiInputPolicy')\n",
        "    class_name = type(PPO).__qualname__\n",
        "    plot_key = f'{class_name}_rewards_'+step_key\n",
        "    try:\n",
        "        rewards, orders = train_val_model(PPO, policy, env_train, env_testing, seed, step_key,  \n",
        "                                                    learning_rate, gamma, ent_coef, total_learning_timesteps)\n",
        "    except Exception as e:\n",
        "        print(f'''there was an error {e} with those parameters: timesteps: {learning_timesteps}, \\n\n",
        "              ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    # timesteps_models_dict[step_key] = models_dict\n",
        "    min_rewards, avg_rewards, max_rewards, = print_stats(rewards, 'Reward')\n",
        "    print_stats(orders, 'Orders')\n",
        "    label = f'Avg. {avg_rewards:>7.2f} : {class_name} - {step_key}'\n",
        "    plot_data[plot_key] = rewards\n",
        "    plot_settings[plot_key] = {'label': label}\n",
        "    params['avg_orders'] = np.mean(orders)       \n",
        "\n",
        "    return {'loss': -avg_rewards, 'status': STATUS_OK, 'eval_time': time.time(), 'parameters': params}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results for best models from hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how does the model perform on the following week\n",
        "hyperparameter_tune_weekly_result_next_week = []\n",
        "results_df_success_negative_no_zero = results_df_success_negative_sorted[results_df_success_negative_sorted['iteration'] == 7]\n",
        "for row in range(0, len(results_df_success_negative_no_zero)):\n",
        "    print(f\"{'-'*40} loss: {round(results_df_success_negative_no_zero.iloc[row, 0], 2)} {'-'*40}\")\n",
        "    i = results_df_success_negative_no_zero.iloc[row, 3] - 7    \n",
        "    parameters = {\n",
        "        'learning_rate': results_df_success_negative_no_zero.iloc[row, 4],\n",
        "        'ent_coef': results_df_success_negative_no_zero.iloc[row, 5],\n",
        "        'gamma': results_df_success_negative_no_zero.iloc[row, 6]\n",
        "    }\n",
        "    result = objective_testing(parameters)\n",
        "    hyperparameter_tune_weekly_result_next_week.append(result)\n",
        "    # print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameter_tune_weekly_result_next_week[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameter_tune_weekly_result_next_week_copy = hyperparameter_tune_weekly_result_next_week.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_hyperparameter_tuned = pd.DataFrame()\n",
        "new_dict = {}\n",
        "for idx, result in enumerate(hyperparameter_tune_weekly_result_next_week_copy):\n",
        "    new_dict['loss'] = result['loss']\n",
        "    new_dict['learning_rate'] = result['parameters']['learning_rate']\n",
        "    new_dict['ent_coef'] = result['parameters']['ent_coef']\n",
        "    new_dict['gamma'] = result['parameters']['gamma']\n",
        "    try:\n",
        "        new_dict['orders'] = result['parameters']['avg_orders']\n",
        "    except: \n",
        "        new_dict['orders'] = 0\n",
        "    new_row = pd.DataFrame(new_dict, index=[idx])\n",
        "    results_df_hyperparameter_tuned = pd.concat([results_df_hyperparameter_tuned, new_row], axis=0)\n",
        "results_df_hyperparameter_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_hyperparameter_tuned.loc[:, 'loss_binary'] = np.where(results_df_hyperparameter_tuned['loss'] > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select only 'loss', 'entropy', gamma, and learning rate from results_df_success_negative_sorted\n",
        "results_df_success_negative_sorted_sel = results_df_success_negative_sorted.loc[:, ['loss', 'ent_coef', 'gamma', 'learning_rate']]\n",
        "# rename loss to loss_original\n",
        "results_df_success_negative_sorted_sel = results_df_success_negative_sorted_sel.rename(columns={'loss': 'loss_original'})\n",
        "# join results_df_success_negative_sorted_sel on 'entropy', gamma, and learning rate with results_df_hyperparameter_tuned\n",
        "print(len(results_df_hyperparameter_tuned))\n",
        "results_df_hyperparameter_tuned_joined = results_df_hyperparameter_tuned.merge(results_df_success_negative_sorted_sel, how='inner', on=['ent_coef', 'gamma', 'learning_rate']) #, right_on=['ent_coef', 'gamma', 'learning_rate']\n",
        "print(len(results_df_hyperparameter_tuned_joined))\n",
        "results_df_hyperparameter_tuned_joined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_hyperparameter_tuned.to_excel('best_hyperparameter_search_results.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results_df_hyperparameter_tuned.to_excel('best_hyperparameter_search_results.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(4, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_hyperparameter_tuned_joined, x='gamma', y='loss', hue='loss_binary', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Loss')\n",
        "# create the second subplot with gamma x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_hyperparameter_tuned_joined, x='ent_coef', y='loss', hue='loss_binary', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Entropy x Loss')\n",
        "# create the third subplot with learning rate x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_hyperparameter_tuned_joined, x='learning_rate', y='loss', hue='loss_binary', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Loss')\n",
        "# comparing the original loss to new loss\n",
        "sns.scatterplot(data=results_df_hyperparameter_tuned_joined, x='loss', y='loss_original', hue='loss_binary', ax=axs[3], palette=custom_palette)\n",
        "axs[3].set_title('Loss x Loss Old')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = results_df_success_negative_sorted.iloc[1,4]\n",
        "entropy = results_df_success_negative_sorted.iloc[1,5]\n",
        "gamma = results_df_success_negative_sorted.iloc[1,6]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

# Background research

ctrl+L for to do list
[ ] 

{type: newday}
[2024-03-09]
Notes:
Ideas:
TODO:
Activity Log:

This is a summary by Merlin from: https://wiki.pathmind.com/deep-reinforcement-learning
Deep Reinforcement Learning:
- The combination of artificial neural networks and reinforcement learning enables software agents to learn how to achieve complex objectives.
- Neural networks combined with reinforcement learning algorithms have produced astounding results, such as Deepmind’s AlphaGo.

Reinforcement Learning Algorithms:
- Reinforcement learning algorithms are goal-oriented and can achieve superhuman performance under the right conditions.
- These algorithms incorporate deep neural networks and have achieved significant progress in real-life environments.

Applications of Deep Reinforcement Learning:
- Deep reinforcement learning has been applied in various industries, such as industrial robotics, supply chain optimization, and autonomous control systems technology.
- Major companies like Google and Microsoft are utilizing deep reinforcement learning for diverse applications.

Basic Terms of Reinforcement Learning:
- The essential terms of reinforcement learning include agents, environments, states, actions, rewards, policies, and values.
- These terms form the basis for understanding how reinforcement learning algorithms operate.

Discount Factor:
- The discount factor is used to lessen the impact of future rewards on the agent’s choice of action.
- It enforces a form of short-term hedonism and evaluates the present value of future rewards.

State and Reward:
- A state represents a concrete and immediate situation in which the agent finds itself, while a reward reflects the success or failure of an agent's actions.
- Both states and rewards play crucial roles in shaping the agent's decision-making process.

Policy and Value:
- The policy maps states to actions, guiding the agent to the actions promising the highest rewards.
- Value, on the other hand, represents the expected long-term return with discount and evaluates rewards further into the future.

Application of Q-Value:
- The Q-value, or action-value, considers the expected long-term return with discount and includes the current action.
- It plays a significant role in evaluating the value of actions in reinforcement learning scenarios.

Q and Policy:
- Q maps state-action pairs to rewards.
- Policy denotes an action taken under a given state.

Trajectory and Key Distinctions:
- Trajectory influences states through sequences of actions.
- Value and reward differ in time horizons and expectations.

Environment and Agent Functions:
- Environments transform current state actions into new states and rewards.
- Agents transform new states and rewards into next actions.

Reinforcement Learning vs. Other Learning Types:
- Reinforcement learning focuses on long-term rewards and goal-oriented actions.
- It differs from supervised and unsupervised learning in its interpretation of inputs.

Domain Selection for Reinforcement Learning:
- Algorithms must decide which inputs to consider, known as domain selection.
- Domain selection requires human decisions based on problem knowledge or theories.

State-Action Pairs & Probability Distributions:
- Reinforcement learning aims to rank and assign values to state-dependent actions.
- The Q function maps state-action pairs to probable rewards.

Complex Probability Distribution & Statistics:
- Reinforcement learning attempts to model a complex probability distribution of rewards.
- It resembles the problem that inspired the invention of the Monte Carlo method.

Iterative Nature of Reinforcement Learning:
- Reinforcement learning is iterative and learns relations through repetition.
- It involves a tension between exploitation of known rewards and exploration for new actions.

Advantages of Algorithms Over Humans:
- Reinforcement learning algorithms have the potential to learn more and better than humans, as they can run in parallel on many chips, train night and day without fatigue, and learn from a large number of game plays.

Role of Neural Networks in Reinforcement Learning:
- Neural networks are used as function approximators to handle large state or action spaces in reinforcement learning.
- They can be used to approximate a value function or a policy function, learning to map states to values or state-action pairs to Q values.

Convolutional Networks in Reinforcement Learning:
- Convolutional networks are used to recognize an agent's state when the input is visual, such as in game environments.
- In reinforcement learning, they rank the actions possible to perform in a state, deriving different interpretations from images compared to supervised learning.

Application of Q Function in Reinforcement Learning:
- The Q function selects the state-action pair with the highest Q value, taking into account both immediate rewards and delayed rewards in the sequence.
- It is recursive in nature and is adjusted by feedback from the environment, analogous to the backpropagation of error in supervised learning.

Real-World Applications of Reinforcement Learning:
- Reinforcement learning is applied to real-world processes in areas like robotics, industrial operations, traffic control, and recommender systems, performing tactical and strategic tasks.

Parallelization in Accelerating Algorithms:
- Technological advancements enable the acceleration of algorithms by parallelizing computation over multiple chips, effectively collapsing time and increasing the performance of algorithms.
- OpenAI demonstrated the success of parallelization by training an algorithm to play a video game for 10 months, equivalent to 180 years worth of games, which led to the algorithm's victory over a world-champion human team.

Algorithm vs Individual Humans:
- Learning algorithms collect knowledge similar to how humans do through language and reports back.
- Civilization's accumulated wisdom is pitted against a single human.

Resources for Reinforcement Learning:
- Reinforcement Learning books: Richard Sutton and Andrew Barto's 'Reinforcement Learning: An Introduction'
- Survey papers on Reinforcement Learning.

Reinforcement Learning Methods:
- Dynamic Programming, Monte Carlo, Temporal-Difference, Q-Learning, Sarsa, and R-Learning are fundamental methods.
- Function Approximation and Policy Search/Policy Gradient are key approaches.

Policy Search and Hierarchical RL:
- Policy Search methods like Natural Actor-Critic and Relative Entropy Policy Search are important for motor primitives in robotics.
- Hierarchical RL is also a significant area in reinforcement learning.

Reinforcement Learning Papers/Thesis:
- Foundational papers and methods papers, including DP, Monte Carlo, Temporal-Difference, and Policy Search.
- Research highlights the various algorithmic approaches in RL.

Summary of Key RL Lectures:
- Identifies important lectures, courses, and books on RL offered by various universities and institutions.
- Reinforcement Learning by David Silver and CS229 Machine Learning by Andrew Ng are some of the key lectures mentioned.

Subversion and Noise in Collective Models:
- Subversion and noise introduce challenges in collective models.
- This is important to consider when analyzing the success of learning algorithms vs. individual humans.

FURTHER READING and Other Posts:
- Includes references for related reading and other posts available on the Pathmind Wiki.
- Provides resources for further understanding and exploration of the topic.

Temporal Abstraction in Reinforcement Learning:
- DPs and Semi-MDPs provide a framework for temporal abstraction in reinforcement learning.
- This framework allows for the transfer of skills in reinforcement learning.

Deep Learning + Reinforcement Learning:
- Recent works include Human-level Control through Deep Reinforcement Learning (by V. Mnih et al.) published in Nature, 2015.
- Another notable work is Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning by Xiaoxiao Guo et al.

End-to-End Training of Deep Visuomotor Policies:
- This work by Sergey Levine et al. focuses on deep visuomotor policies.
- It was published in ArXiv on 16th October 2015.

Game Playing with Reinforcement Learning:
- Reinforcement learning applications include game plays like backgammon, chess, Flappy Bird, and MarI/O learning to play Mario.
- OpenAI also explored reinforcement learning with Montezuma’s Revenge.

Robotics with Reinforcement Learning:
- Reinforcement learning has been applied to robotics for applications such as locomotion, skill coordination, and autonomous skill acquisition on mobile manipulators.
- It has also been used for policy search and adaptation in robotics.

Control with Reinforcement Learning:
- Reinforcement learning has found applications in aerobatic helicopter flight, autonomous helicopter control, and operations research for product delivery.
- It has also been applied to human-computer interaction for dialogue management optimization.

From Domino.ai - https://domino.ai/blog/deep-reinforcement-learning
Domino's Spring ’24 Release:
- Announcement of the Spring ’24 Release of Domino's offerings.
- Features and improvements in the latest release to be showcased.

Deep Reinforcement Learning:
- An introduction to the concept and its significance in machine learning.
- Overview of the deep Q-learning model and its application in a simulated video game environment.

Agent-Environment Interaction:
- Modeling interactions between the agent and the environment in deep reinforcement learning.
- Using Keras and OpenAI Gym to create a deep learning model for training.

Theory of Reinforcement Learning:
- Explanation of the reinforcement learning paradigm involving an agent and environment.
- Understanding of rewards, states, and sequential decision-making in reinforcement learning.

The Cart-Pole Game:
- Explanation of the classic problem and its objectives.
- Description of the game rules and the reward system.

Markov Decision Processes:
- Mathematical definition of reinforcement learning problems as Markov decision processes.
- Components and concepts involved in Markov decision processes.

Reinforcement Learning Applications:
- Examples and applications of reinforcement learning in various domains.
- Utilization of reinforcement learning in autonomous vehicles, board games, and robot-arm manipulation tasks.

Future of Deep RL Agents:
- Exploration of advanced deep RL agents beyond deep Q-learning.
- Approaches for optimizing the performance of deep reinforcement learning agents.

States and Actions:
- States in reinforcement learning are the conditions or configurations of the environment, while actions are the possible moves or decisions that the agent can make.
- The potential reward for a specific state-action pair is denoted as R, and it represents a probability distribution.

Reward Distribution:
- The reward distribution R determines the expected reward for a state-action pair and is concealed from the agent.
- The specifics of the reward distribution can be discerned by taking actions within the environment.

Next State Probabilities:
- The probability distribution P indicates the likelihood of the next state given a current state-action pair and is also hidden from the agent.
- Aspects of the next state probabilities can be inferred through actions in the environment.

Discount Factor:
- The discount factor γ influences the significance of future rewards, allowing for immediate rewards to be valued more highly than distant ones.
- It prevents value functions from becoming unbounded in games with an infinite number of possible future timesteps.

Optimal Policy:
- The objective of an MDP is to find a policy function π that enables the agent to take the best action for any state encountered.
- The optimal policy π* provides the action that leads to the maximum possible discounted future reward.

Deep Q-Learning Networks:
- Computing the maximum cumulative discounted future reward is computationally intractable, leading to the need for shortcuts like Q-learning.
- Value functions, such as Vπ(s), gauge the value of a state when following a policy.

Estimating Optimal Actions:
- The Q-learning approach estimates the optimal action for a given situation, accounting for the abundance of possible future outcomes.
- It utilizes value functions to indicate the value of a state when following a policy.

Value Functions:
- The value function, V π (s), represents the utility of a particular state s and is high for states where the pole is near vertical.
- In contrast, for a state where the pole angle is approaching horizontal, V π (sh) is lower, indicating that the episode is likely to terminate within a few timesteps.

Q-Value Functions:
- The Q-value function, Qπ (s, a), considers the utility of a particular action when paired with a given state.
- For example, pairing the action left with a particular state generally corresponds to a higher cumulative discounted future reward compared to pairing the action right.

Estimating Optimal Q-Value:
- When encountering a state s, we aim to calculate the optimal Q-value, Q∗ (s, a).
- With deep Q-learning networks, we can use an artificial neural network to estimate the optimal Q-value by leveraging the approximation equation.

Defining a DQN Agent:
- A DQN agent is defined for learning how to act in an environment, such as the Cart-Pole game from the OpenAI Gym library of environments.
- The agent incorporates hyperparameters and a neural network model to facilitate learning from the environment.

Initialization Parameters:
- The DQNAgent class is initialized with various parameters such as state_size, action_size, memory, gamma, epsilon, epsilon_decay, epsilon_min, learning_rate, and the model architecture.
- The model architecture includes layers with specific activation functions and input dimensions.

Training and Execution:
- The training process involves updating the agent's memory, training the neural network model, and adjusting the agent's exploration-exploitation strategy based on observed rewards and states.
- The act method is used to determine the agent's action based on the current state.

Saving Model Parameters:
- The agent includes a method for saving the neural network's weights with a specified name for future use.

Dependency and Environment Setup:
- The agent relies on dependencies such as gym, numpy, and keras for reinforcement learning tasks.
- The Cart-Pole environment from OpenAI Gym is utilized, and hyperparameters are set for training the neural network model.

Parameters:
- State size and action size are environment-specific, with values of 4 and 2 for the Cart-Pole game, respectively.
- The memory is for storing and replaying memories, maintained as a deque with a maximum length of 2000.

Hyperparameters:
- Gamma (discount factor) is an agent hyperparameter that discounts future rewards with typical values near 1, 0.9, 0.95, 0.98, or 0.99.
- Epsilon (exploration rate) represents the proportion of random actions for exploration and is decayed gradually over gameplay.

Decay and Learning Rate:
- Epsilon decay gradually reduces the exploration rate with common options of 0.990, 0.995, and 0.999.
- Learning rate is the stochastic gradient descent hyperparameter.

Neural Network Model:
- The _build_model() method constructs and compiles a Keras neural network for mapping environmental states to the agent's Q-values.
- It includes hidden layers with ReLU neurons, and the output layer corresponds to the number of possible actions.

Gameplay Memory:
- The agent's remember() method appends memories of state, action, reward, next state, and a Boolean flag for episode completion to the memory deque.
- Training of the neural net model occurs through replaying memories sampled from the memory deque.

Training Process:
- Sampling a minibatch of memories for more efficient model training by providing a richer range of experiences.
- Model training involves rounds of training for each sampled memory, particularly considering the done flag for episode completion.

Conclusion:
- The DQNAgent class includes various method implementations to facilitate reinforcement learning in the Cart-Pole environment.
- Efficient memory replay and training processes are integral to the agent's learning and decision-making capabilities.

Target Reward Calculation:
- Calculating the highest possible reward from the timestep as the target reward when the episode is complete.
- Estimating the target reward by adding the discounted maximum future Q-value when the episode is not completed.

Training the Model:
- Using the predict method to estimate the future Q-values and store them in target_f.
- Training the model by replacing the target_f output and calling the fit method.

Selecting an Action to Take:
- Using the act method to select either a random exploratory action or an exploitative action based on a random value.
- Selecting an action that exploits the knowledge learned by the model via memory replay.

Saving and Loading Model Parameters:
- Utilizing save and load methods to save and load the model parameters at regular intervals to maintain performance.

Interacting with an OpenAI Gym Environment:
- Initializing an instance of the DQN agent to interact with an OpenAI Gym environment such as the Cart-Pole game.

Gameplay Loop:
- Initiating multiple rounds of game play, episode by episode, using a for loop.
- Engaging in actions such as resetting the game, rendering the environment, and training the agent.

Episode Completion and Reward:
- If the episode is not done, the reward increases by one for each additional time step of gameplay.
- The episode is represented by a while loop that iterates over the time steps until the episode ends.

Agent's Behavior within an Episode:
- The state is reoriented to a row at the start of the episode using reshape.
- The agent uses the remember() method to save all aspects of the time step to memory, including the state, action, reward, next state, and the 'done' flag.

Agent Training and Neural Net Parameters:
- The agent uses the train() method to train its neural net parameters by replaying its memories of gameplay.
- Every 50 episodes, the agent uses the save() method to store the neural net model's parameters.

Agent Performance and Exploration Rate:
- The exploration rate ε starts at 100 percent and decays gradually over episodes.
- The agent improves its performance significantly over episodes, attaining a perfect score of 199 in the final 10 episodes.

Hyperparameter Optimization with SLM Lab:
- SLM Lab is a deep reinforcement learning framework developed by Wah Loon Keng and Laura Graesser.
- It provides modular agent components and allows experimenting with various agent hyperparameters.

Metrics for Evaluating Model Performance:
- SLM Lab provides metrics such as strength, speed, stability, consistency, and fitness for evaluating model performance.

Optimal Hyperparameter Settings:
- For a DQN agent playing the Cart-Pole game, optimal hyperparameter settings include a single-hidden-layer neural net architecture with 64 neurons, the tanh activation function, and a low learning rate.
- Trials with an exploration rate that anneals over 10 episodes outperform trials that anneal over 50 or 100 episodes.

Agents Beyond DQN:
- Deep Q-learning networks, like DQNs, are relatively simple in the world of deep reinforcement learning.
- DQNs make efficient use of training samples, but they do have drawbacks, especially for environments with a large number of state-action pairs.

Complexity of Q-function:
- In some cases, the Q-function can become extremely complex, making it intractable to estimate the optimal Q-value, Q∗.
- This complexity can make DQNs inefficient at exploring and converging on Q∗.

Types of Agents Beyond DQNs:
- The main categories of deep RL agents include value optimization, imitation learning, model optimization, and policy optimization.
- These agents solve reinforcement learning problems through different approaches such as predicting future states and learning policies directly.

Policy Gradient Algorithms:
- Policy gradient algorithms like REINFORCE perform gradient ascent on π directly, leading to more widely applicable solutions than value optimization methods like DQN.
- However, they have higher variance in performance and require a larger number of training samples.

The Actor-Critic Algorithm:
- The actor-critic algorithm combines the value optimization and policy optimization approaches, involving a loop between the actor and critic.
- It has the advantage of solving a broader range of problems than DQN while having lower variance in performance relative to REINFORCE.

Chapter Summary:
- The chapter covers the essential theory of reinforcement learning, including deep Q-learning and its application to the Cart-Pole environment.
- It also introduces deep RL algorithms beyond DQN, such as REINFORCE and actor-critic.

Related Tags:
- The blog also covers related tags such as machine learning, practical techniques, Keras models, and others.
- Topics include deep Q-learning, DQN, reinforcement learning, and deep learning.

Reinforcement Learning (RL) can be a powerful complement to your genetic algorithm, especially in optimization problems where the goal is to find the best parameters or strategies over time. Given your setup, where you're dividing data into training, validation, and test sets, here are several RL methods that could work well with your genetic algorithm:

### 1. Deep Q-Learning (DQN)

**Why it works well:**
- **Continuous Action Space:** DQN can handle continuous action spaces, which is beneficial if your genetic algorithm involves parameters that can take on a range of values.
- **Experience Replay:** It uses a replay buffer to store past experiences, allowing the agent to learn from a variety of situations, which can be particularly useful if your genetic algorithm involves complex interactions or dependencies between parameters.
- **Parallelization:** DQN can be parallelized, making it efficient to train on large datasets.

### 2. Proximal Policy Optimization (PPO)

**Why it works well:**
- **Stability:** PPO is designed to be more stable than other RL methods, making it less likely to overfit to the training data.
- **Efficiency:** It uses a surrogate objective function to update the policy, which can lead to more efficient learning.
- **Robustness:** PPO is robust to changes in the environment, which can be beneficial if your genetic algorithm needs to adapt to new data or parameters.

### 3. Advantage Actor-Critic (A2C)

**Why it works well:**
- **Online Learning:** A2C is an online learning algorithm, meaning it can learn from each new piece of data without needing to retrain from scratch. This can be particularly useful if your genetic algorithm is iterative and you want to incorporate new data or feedback into the learning process.
- **Simplicity:** A2C is relatively simple to implement and understand, which can make it easier to integrate with your existing genetic algorithm.
- **Performance:** It has been shown to perform well in a variety of tasks, making it a versatile choice for optimization problems.

### 4. Soft Actor-Critic (SAC)

**Why it works well:**
- **Stability and Robustness:** SAC is designed to be stable and robust, making it less likely to overfit to the training data and more capable of handling complex environments.
- **Sample Efficiency:** It uses a maximum entropy framework to encourage exploration, which can lead to more efficient learning.
- **Adaptability:** SAC is adaptable to different tasks and environments, making it a good choice if your genetic algorithm needs to adapt to new data or parameters.

### 5. Deep Deterministic Policy Gradient (DDPG)

**Why it works well:**
- **Continuous Action Space:** Like DQN, DDPG can handle continuous action spaces, which is beneficial if your genetic algorithm involves parameters that can take on a range of values.
- **Stability:** DDPG is designed to be stable, making it less likely to overfit to the training data.
- **Efficiency:** It uses a deterministic policy, which can lead to more efficient learning.

Each of these methods has its own strengths and weaknesses, and the best choice will depend on the specifics of your genetic algorithm and the nature of your optimization problem. It's also worth noting that integrating RL with a genetic algorithm can be complex, and careful consideration will be needed to ensure that the RL component effectively enhances the performance of the genetic algorithm.
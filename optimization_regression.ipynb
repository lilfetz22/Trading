{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random\n",
    "import mplfinance \n",
    "import fx\n",
    "import fx_rl\n",
    "from collections import deque\n",
    "from renkodf import Renko\n",
    "from scipy.signal import lfilter\n",
    "from deap import base, creator, tools\n",
    "from scipy.stats import zscore\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "# df_full['datetime'] = pd.to_datetime(df_full['date'] + ' ' + df_full['time'], format='%Y.%m.%d %H:%M')\n",
    "# filter the data to just 2023\n",
    "\n",
    "# df_2022 = fx.prep_data(df_full, 2022)\n",
    "# df_2023 = fx.prep_data(df_full, 2023)\n",
    "# df_2024 = fx.prep_data(df_full, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Renko variable\n",
    "# initial_brick_size = 0.0003\n",
    "# # create a list of possible brick sizes a max at 0.001 and min at 0.00001 and each step is 0.00001\n",
    "# brick_size_list = np.arange(0.0005, 0.00101, 0.00001)\n",
    "# brick_size_str = str(int(initial_brick_size*10000))\n",
    "\n",
    "# # sma variables\n",
    "# initial_sma_length = 3\n",
    "# sma_length_list = np.arange(3, 20, 1)\n",
    "# initial_smoothing_sma = 3\n",
    "# smoothing_sma_list = np.arange(3, 20, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot Size\n",
    "lot_size = 1\n",
    "per_lot = 100_000\n",
    "\n",
    "# stop loss\n",
    "stop_loss_size_init = 0.00033*4\n",
    "# threshold for entry\n",
    "threshold_init = 0.00033*3\n",
    "\n",
    "# take profit\n",
    "take_profit_size_init = 0.00033*5\n",
    "\n",
    "# Commissions\n",
    "msolutions_commission = lot_size * -5\n",
    "\n",
    "# starting balance\n",
    "balance = 200_000\n",
    "\n",
    "# testing conditions\n",
    "timeframe_list = ['5min', '15min', 'hourly']\n",
    "comparisons = ['5min,15_min,2', '5min,hourly,11', '15min,hourly,3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_5_min_data(time, direction, tp_price, sl_price):\n",
    "    filtered_15min_data = (pl.scan_csv('EURUSD_full_tickstory_data_5_min.csv')\n",
    "        .with_columns(\n",
    "            pl.col('').alias('datetime').str.to_datetime(format='%Y-%m-%d %H:%M:%S'),\n",
    "        ).drop('')\n",
    "        .filter(\n",
    "            (pl.col('datetime') <= time)\n",
    "        )                      \n",
    "    ).rename(lambda col_name: col_name.lower()).sort('datetime').tail(12).collect()\n",
    "\n",
    "    result = filtered_15min_data.select([\n",
    "        pl.col('datetime'),\n",
    "        (((pl.col('high') >= tp_price) & (direction == 'buy')) |\n",
    "         ((pl.col('low') <= tp_price) & (direction == 'sell'))).alias('take_profit_hit'),\n",
    "        (((pl.col('low') <= sl_price) & (direction == 'buy')) |\n",
    "         ((pl.col('high') >= sl_price) & (direction == 'sell'))).alias('stop_loss_hit'),\n",
    "    ]).filter(\n",
    "        pl.col('take_profit_hit') | pl.col('stop_loss_hit')\n",
    "    ).sort('datetime').head(1)\n",
    "\n",
    "    if len(result) == 0:\n",
    "        # print(time, tp_price, sl_price)\n",
    "        return [tp_price, True]\n",
    "\n",
    "    if result.row(0)[1]:\n",
    "        return [tp_price, False]\n",
    "    else: #result.row(0)[2]:\n",
    "        return [sl_price, False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_exit_price(row, full_data) -> pl.List:\n",
    "    position_type, entry_time, stop_loss_price, take_profit_price = row\n",
    "    entry_time = pd.to_datetime(entry_time)\n",
    "\n",
    "    # Filter the full_data to only include rows where 'datetime' is > entry_time\n",
    "    filtered_df = full_data.filter(pl.col('datetime') > entry_time)\n",
    "\n",
    "    # Check all exit conditions\n",
    "    result = filtered_df.select([\n",
    "        pl.col('datetime'),\n",
    "        pl.col('high'),\n",
    "        pl.col('low'),\n",
    "        pl.col('open'),\n",
    "        pl.col('close'),\n",
    "        pl.col('week_transition'),\n",
    "        (((pl.col('high') >= take_profit_price) & (position_type == 'buy')) |\n",
    "         ((pl.col('low') <= take_profit_price) & (position_type == 'sell'))).alias('take_profit_hit'),\n",
    "        (((pl.col('low') <= stop_loss_price) & (position_type == 'buy')) |\n",
    "         ((pl.col('high') >= stop_loss_price) & (position_type == 'sell'))).alias('stop_loss_hit'),\n",
    "        (pl.col('week_transition') == 1).alias('week_transition_hit'),\n",
    "        ((pl.col('datetime').dt.time() == time(0,0,0)) & \n",
    "         (pl.lit(position_type) == 'buy')).alias('swap_hit')\n",
    "    ]).filter(\n",
    "        pl.col('take_profit_hit') | pl.col('stop_loss_hit') | pl.col('week_transition_hit') | pl.col('swap_hit')\n",
    "    ).sort('datetime').head(1)\n",
    "    neither_hit = False\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        return [None, None, neither_hit]\n",
    "\n",
    "    exit_row = result.row(0)\n",
    "    # print(exit_row)\n",
    "    exit_time = exit_row[0]\n",
    "    if exit_row[-4]: # take_profit_hit\n",
    "        if exit_row[-3]:\n",
    "            exit_price, neither_hit = check_5_min_data(exit_row[0], position_type, take_profit_price, stop_loss_price)\n",
    "        exit_price = take_profit_price\n",
    "    elif exit_row[-3]: # stop_loss_hit\n",
    "        exit_price = stop_loss_price\n",
    "    elif exit_row[-2]: # week_transition_hit\n",
    "        exit_price = exit_row[4] # close price\n",
    "    elif exit_row[-1]: # swap_hit\n",
    "        exit_price = exit_row[3] # open price\n",
    "    else:\n",
    "        return [None, None, neither_hit]\n",
    "\n",
    "    return [exit_time, exit_price, neither_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(comparison_str, threshold, take_profit_size, stop_loss_size):\n",
    "    comparison = comparison_str.split(',')\n",
    "    try:\n",
    "        df_joined = (\n",
    "            pl.scan_csv(f\"df_w_news_{comparison[0]}_pred.csv\")\n",
    "            # convert 'Time' to datetime\n",
    "            .with_columns([\n",
    "                pl.col('datetime').str.to_datetime(format='%Y-%m-%d %H:%M:%S'),#T %.f\n",
    "            ])\n",
    "        .sort('ds')\n",
    "            ).collect()\n",
    "    except:\n",
    "        df_joined = (\n",
    "            pl.scan_csv(f\"df_w_news_{comparison[0]}_pred.csv\")\n",
    "            # convert 'Time' to datetime\n",
    "            .with_columns([\n",
    "                pl.col('datetime').str.to_datetime(format='%Y-%m-%dT%H:%M:%S%.f'),#T %.f\n",
    "            ])\n",
    "        .sort('ds')\n",
    "            ).collect()\n",
    "\n",
    "    # df_processed = fcst.preprocess(df_joined, static_features=[])\n",
    "    max_date_1hr = df_joined.select(pl.col('datetime').max()).item()\n",
    "    testing = False\n",
    "    if testing:\n",
    "        train, validation, test = fx_rl.slices_finder_polars(df_joined, max_date_1hr, date_col='datetime', testing_needed=testing)\n",
    "        train = train.select(*df_joined.columns)#.drop(\"datetime\")\n",
    "        validation = validation#.drop(\"datetime\")\n",
    "        test = test#.drop(\"datetime\")\n",
    "    else:\n",
    "        train, validation = fx_rl.slices_finder_polars(df_joined, max_date_1hr, date_col='datetime', testing_needed=testing)\n",
    "        train = train.select(*df_joined.columns)#.drop(\"datetime\")\n",
    "        validation = validation#.drop(\"datetime\")\n",
    "\n",
    "    # create a column called Pred_lag1 which is the lag of the Pred column in polars\n",
    "    df_joined_lag3 = train.with_columns(\n",
    "        pl.col(\"pred\").shift(-int(comparison[2])).alias(\"pred_lag\"),\n",
    "    ).drop_nulls(subset=[\"pred_lag\"])\n",
    "\n",
    "    try: \n",
    "        full_ohlc_hourly_df = (\n",
    "            pl.scan_csv(f'EURUSD_full_tickstory_data_{comparison[1]}.csv')# 15_min\n",
    "            .with_columns([\n",
    "                pl.col('Time').alias('datetime').str.to_datetime(format='%Y-%m-%d %H:%M:%S+00:00'),\n",
    "            ])\n",
    "            .drop('Time')\n",
    "        ).rename(lambda col_name: col_name.lower()).collect()\n",
    "    except:\n",
    "        full_ohlc_hourly_df = (\n",
    "            pl.scan_csv(f'EURUSD_full_tickstory_data_{comparison[1]}.csv')\n",
    "            .with_columns([\n",
    "                pl.col('').alias('datetime').str.to_datetime(format='%Y-%m-%d %H:%M:%S'),\n",
    "            ])\n",
    "            .drop('')\n",
    "        ).rename(lambda col_name: col_name.lower()).collect()  \n",
    "\n",
    "    df_joined_hourly = df_joined_lag3.join(full_ohlc_hourly_df, on='datetime', how='inner')   \n",
    "\n",
    "    # create the buy and sell signals which is that if the Pred_lag1 is greater than the y by threshold then buy and if the \n",
    "    # Pred_lag1 is less than the y by threshold then sell\n",
    "    df_joined_bs = df_joined_hourly.with_columns(\n",
    "        (pl.col(\"pred_lag\") > pl.col(\"y\") + threshold).cast(pl.Int8).alias(\"buy\"),\n",
    "        (pl.col(\"pred_lag\") < pl.col(\"y\") - threshold).cast(pl.Int8).alias(\"sell\"),\n",
    "    )\n",
    "\n",
    "    # create a column called signal which is the buy column if the buy column is 1 and the sell column is 0 \n",
    "    # and the sell column * -1 if the buy column is 0 and the sell column is 1 and 0 if the buy column is 0 and the sell column is 0\n",
    "    df_joined_signal = df_joined_bs.with_columns(\n",
    "        (pl.col(\"buy\") + (pl.col(\"sell\") * -1)).alias(\"signal\"),\n",
    "    )\n",
    "    # adjust signal to have the word 'buy' if it is 1 and 'sell' if it is -1 and \"\" if it is 0 for df_joined_signal which is a polars dataframe\n",
    "    df_joined_signal_txt = df_joined_signal.with_columns(\n",
    "        pl.when(pl.col('signal') == 1)\n",
    "        .then(pl.lit('buy'))\n",
    "        .when(pl.col('signal') == -1)\n",
    "        .then(pl.lit('sell'))\n",
    "        .otherwise(pl.lit(None))\n",
    "        .alias('signal_text'),\n",
    "        # add a column for the day of the week \n",
    "        pl.col('datetime').dt.weekday().alias('day_of_week'),\n",
    "        pl.col('datetime').dt.week().alias('week_of_year'),\n",
    "    )\n",
    "    # add a column to indicate a transition between weeks by placing a 1 in the column if the week of year changes in the next row\n",
    "    df_joined_week_trans = df_joined_signal_txt.with_columns(\n",
    "        pl.when((pl.col('week_of_year') != pl.col('week_of_year').shift(-1)) |\n",
    "        (pl.col('week_of_year') != pl.col('week_of_year').shift(1)) |\n",
    "        (pl.col('week_of_year') != pl.col('week_of_year').shift(2)) |\n",
    "        (pl.col('week_of_year') != pl.col('week_of_year').shift(3)))\n",
    "        .then(pl.lit(1))\n",
    "        .otherwise(pl.lit(0))\n",
    "        .alias('week_transition')\n",
    "    )\n",
    "\n",
    "    # if the week_transition is 1 and signal is not 0 then replace signal_text with 'exit' for the df_joined_week_trans polars dataframe\n",
    "    df_joined_week_trans_exit = df_joined_week_trans.with_columns(\n",
    "        pl.when(\n",
    "            (pl.col(\"week_transition\") == 1) & (pl.col(\"signal\") != 0)\n",
    "        )\n",
    "        .then(pl.lit(\"exit\"))\n",
    "        .otherwise(pl.col(\"signal_text\")).alias(\"signal_text\")\n",
    "    )\n",
    "\n",
    "    # filter to those rows where the signal_text is not null, exit\n",
    "    positions = df_joined_week_trans_exit.filter(\n",
    "        (pl.col('signal_text').is_not_null()) & \n",
    "        (pl.col('signal_text') != 'exit') & \n",
    "        ((abs(pl.col('seconds_since_last_news_event')) > 900) &\n",
    "        (abs(pl.col('seconds_to_next_news_event')) > 900)) &\n",
    "        ((pl.col('signal_text').shift().is_null()) | (pl.col('signal_text').shift() != pl.col('signal_text')))\n",
    "        ).with_columns(\n",
    "        pl.when(pl.col('signal_text') == 'buy')\n",
    "        .then(pl.col('close') + take_profit_size)\n",
    "        .when(pl.col('signal_text') == 'sell')\n",
    "        .then(pl.col('close') - take_profit_size)\n",
    "        .otherwise(None)\n",
    "        .alias('take_profit'),\n",
    "        pl.when(pl.col('signal_text') == 'buy')\n",
    "        .then(pl.col('close') - stop_loss_size)\n",
    "        .when(pl.col('signal_text') == 'sell')\n",
    "        .then(pl.col('close') + stop_loss_size)\n",
    "        .otherwise(None)\n",
    "        .alias('stop_loss')\n",
    "        ).select(\n",
    "            pl.col('signal_text').alias('direction'),\n",
    "            pl.col('datetime').alias('entry_time'),\n",
    "            pl.col('close').alias('entry_price'),\n",
    "            pl.col('take_profit'),\n",
    "            pl.col('stop_loss'),\n",
    "        )\n",
    "\n",
    "    exit_times = []\n",
    "    exit_prices = []\n",
    "    neither_hit_bool = []\n",
    "    for row in positions.iter_rows():\n",
    "        exit_time, exit_price, neither_hit = find_exit_price([row[0], row[1], row[4], row[3]], df_joined_week_trans_exit)\n",
    "        exit_times.append(exit_time)\n",
    "        exit_prices.append(exit_price)\n",
    "        neither_hit_bool.append(neither_hit)\n",
    "        \n",
    "    positions_with_exit_info = positions.with_columns([\n",
    "        pl.Series(\"exit_time\", exit_times),\n",
    "        pl.Series(\"exit_price\", exit_prices),\n",
    "        pl.Series(\"neither_hit\", neither_hit_bool)\n",
    "    ])\n",
    "\n",
    "    # profit calculations\n",
    "    profit_df = positions_with_exit_info.with_columns(\n",
    "        pl.when(pl.col('direction') == 'buy')\n",
    "        .then(\n",
    "            (((pl.col(\"exit_price\") - pl.lit(max(0., np.random.normal(0.0001, 0.00003)))) - \n",
    "            (pl.col(\"entry_price\") + pl.lit(max(0., np.random.normal(0.0001, 0.00003))))) * per_lot * lot_size) + msolutions_commission\n",
    "        )\n",
    "        .otherwise(\n",
    "            (((pl.col(\"entry_price\") - pl.lit(max(0., np.random.normal(0.0001, 0.00003)))) -\n",
    "            (pl.col(\"exit_price\") + pl.lit(max(0., np.random.normal(0.0001, 0.00003)))))  * per_lot * lot_size) + msolutions_commission\n",
    "            ).alias(\"profit\")\n",
    "    )\n",
    "    profit_df_daily = profit_df.group_by_dynamic('entry_time', every='1d').agg(\n",
    "        pl.col('profit').sum().alias('profit')\n",
    "    )\n",
    "    profit_df_daily_drawdown = profit_df_daily.filter(pl.col('profit') < 0)\n",
    "    \n",
    "    total_profit = profit_df_daily['profit'].sum()\n",
    "\n",
    "    profit_factor = total_profit / abs(profit_df_daily_drawdown['profit'].sum())\n",
    "\n",
    "    return total_profit, len(profit_df_daily_drawdown), profit_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lol = [brick_size_list, sma_length_list, smoothing_sma_list] # , lot_sizes_list,  \n",
    "# def constraint_handler(individual):\n",
    "#     counter = 0\n",
    "#     for i, lst in zip(individual, lol):\n",
    "#         # Ensure parameter is greater than   0\n",
    "#         if (i <= 0) | ((counter == len(individual) - 1) & (i < 2)):\n",
    "#             individual[counter] = np.random.choice(lst)  # Reset to a valid value\n",
    "#         counter += 1\n",
    "#     return individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fitness function\n",
    "import sys\n",
    "# Check if FitnessMax already exists in the __main__ namespace\n",
    "if 'FitnessMax' not in sys.modules['__main__'].__dict__:\n",
    "    # If not, create it\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "\n",
    "if 'Individual' not in sys.modules['__main__'].__dict__:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bca6ba3c0ae44c796c6afa70f58ecb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min,hourly,11 0.005945469692266635 0.006899222979130406 0.00926936102282228\n",
      "-5513.998141447595 35 -0.3443706817929099\n",
      "1\n",
      "5min,hourly,11 0.0037638279533541274 0.00550550299756155 0.0075293435421130575\n",
      "-13454.377796573777 101 -0.2681499277387649\n",
      "2\n",
      "15min,hourly,3 0.009492806430667041 0.0026271570674927074 0.0010287743828694313\n",
      "588.2191394783943 1 4.45551940923866\n",
      "3\n",
      "5min,15_min,2 0.0025731725317836313 0.00540888947315219 0.004999449611053709\n",
      "1127.3602769809047 41 0.05672629122501704\n",
      "4\n",
      "15min,hourly,3 0.008979891579685002 0.007479269614572749 0.0020544144387871226\n",
      "980.6310254859986 2 2.1177516460262176\n",
      "5\n",
      "5min,15_min,2 6.290364912541839e-05 0.0007194979081264168 0.002137842333616701\n"
     ]
    }
   ],
   "source": [
    "population_size = 100\n",
    "global_total_profits = []  # A list to collect total profits from all individuals\n",
    "global_days_in_drawdowns = []  # A list to collect days in drawdown from all individuals\n",
    "# Global variables to keep track of the number of evaluations\n",
    "num_evaluations = 0\n",
    "discard_threshold = 20  # Discard the first 20 evaluations\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Register an attribute generator for each parameter with its own range\n",
    "toolbox.register(\"take_profit\", np.random.uniform, low=1e-10, high=0.01)\n",
    "toolbox.register(\"stop_loss\", np.random.uniform, low=1e-10, high=0.01)\n",
    "toolbox.register(\"threshold\", np.random.uniform, low=1e-10, high=0.01)\n",
    "toolbox.register(\"comparison\", np.random.choice, comparisons)\n",
    "\n",
    "# Combine the attribute generators to create an individual\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                  (toolbox.comparison, toolbox.take_profit, toolbox.stop_loss, toolbox.threshold))# , toolbox.lot_size, \n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "# initialize a dataframe for the each evaluation\n",
    "eval_df = pl.DataFrame(['comparison', 'take_profit', 'stop_loss', 'threshold', 'total_profit', 'days_in_drawdown', 'profit_factor', 'num_evals'])\n",
    "\n",
    "\n",
    "def eval_func(individual):\n",
    "    # Assuming these are global variables accessible within the scope of eval_func\n",
    "    global global_total_profits  # A list to collect total profits from all individuals\n",
    "    global global_days_in_drawdowns  # A list to collect days in drawdown from all individuals\n",
    "    global num_evaluations  # A global variable to keep track of the number of evaluations\n",
    "    global eval_df\n",
    "\n",
    "    # Unpack individual parameters\n",
    "    comparison, take_profit, stop_loss, threshold = individual #, lot_size, brick_size\n",
    "    print(comparison, take_profit, stop_loss, threshold) # , lot_size, brick_size\n",
    "\n",
    "    # Perform calculations using these parameters\n",
    "    total_profit, days_in_drawdown, profit_factor = objective(comparison, take_profit, stop_loss, threshold)\n",
    "    print(total_profit, days_in_drawdown, profit_factor)\n",
    "\n",
    "    # Update the global lists with the new values\n",
    "    global_total_profits.append(total_profit)\n",
    "    # print(global_total_profits)\n",
    "    global_days_in_drawdowns.append(days_in_drawdown)\n",
    "    \n",
    "    num_evaluations +=  1\n",
    "    # save the results to a dataframe\n",
    "    try:\n",
    "        eval_df = pl.concat([eval_df, pl.DataFrame({'comparison': comparison, 'take_profit': take_profit, 'stop_loss': stop_loss, 'threshold': threshold,\n",
    "                                                    'total_profit': total_profit, 'days_in_drawdown': days_in_drawdown, 'profit_factor': profit_factor,\n",
    "                                                    'num_evals': num_evaluations} \n",
    "                                                    )])\n",
    "    except:\n",
    "        eval_df = pl.DataFrame({'comparison': comparison, 'take_profit': take_profit, 'stop_loss': stop_loss, 'threshold': threshold,\n",
    "                                                    'total_profit': total_profit, 'days_in_drawdown': days_in_drawdown, 'profit_factor': profit_factor,\n",
    "                                                    'num_evals': num_evaluations}\n",
    "                                                    )\n",
    "\n",
    "\n",
    "    if len(global_total_profits) > discard_threshold:\n",
    "        # Normalize total_profit and days_in_drawdown independently\n",
    "        norm_total_profit = zscore(global_total_profits)[-1]\n",
    "        norm_days_in_drawdown = zscore(global_days_in_drawdowns)[-1]\n",
    "        # Combine the scores\n",
    "        score = norm_total_profit - norm_days_in_drawdown\n",
    "        if norm_total_profit == 0:\n",
    "            score = 0\n",
    "        print(score)\n",
    "        return (score,)\n",
    "    else:\n",
    "        print(num_evaluations)\n",
    "        return (0,)\n",
    "\n",
    "toolbox.register(\"evaluate\", eval_func)\n",
    "\n",
    "# Define the feasibility function\n",
    "# def feasible(individual):\n",
    "#     for i, lst in zip(individual, lol):\n",
    "#         # find the min of the list\n",
    "#         min = lst.min()\n",
    "#         # find the max of the list\n",
    "#         max = lst.max()\n",
    "#         if i < min or i > max:\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# # Define a distance function to the feasibility region\n",
    "# def distance(individual):\n",
    "#     distances = 0\n",
    "#     for i, lst in zip(individual, lol):\n",
    "#         # find the min of the list\n",
    "#         min = lst.min()\n",
    "#         # find the max of the list\n",
    "#         max = lst.max()\n",
    "#         if i < min:\n",
    "#             # find out how far away from the min the parameter is and then normalize it\n",
    "#             dist = abs(i - min) / (max - min)\n",
    "#         elif i > max:\n",
    "#             # find out how far away from the max the parameter is and then normalize it\n",
    "#             dist = abs(i - max) / (max - min)\n",
    "#         else:\n",
    "#             dist = 0\n",
    "#         distances += dist\n",
    "#     return distances  # Distance from the feasibility region\n",
    "\n",
    "# # Decorate the evaluation function with a DeltaPenalty decorator\n",
    "# toolbox.decorate(\"evaluate\", tools.DeltaPenalty(feasible, 1.0, distance))\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "# toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=0.00001, up=0.00101, eta=20.0, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "# Register the population and other operators as before\n",
    "# toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)\n",
    "\n",
    "# Create initial population\n",
    "population = toolbox.population(n=population_size)\n",
    "for ind in population:\n",
    "    ind.fitness.values = (0,)  # Temporary fitness value\n",
    "\n",
    "# Initialize the logbook to store statistics\n",
    "logbook = tools.Logbook()\n",
    "\n",
    "# Register the statistics you want to track\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "# stats.register(\"gen\", np.mean)\n",
    "stats.register(\"average\", np.mean)\n",
    "stats.register(\"minimum\", np.min)\n",
    "stats.register(\"maximum\", np.max)\n",
    "stats.register(\"stdeviation\", np.std)\n",
    "logbook.header = \"gen\", \"avg\", \"min\", \"max\", \"std\"\n",
    "\n",
    "# Define the convergence criterion\n",
    "convergence_threshold = 0.01 # Example threshold for fitness improvement\n",
    "max_generations_without_improvement = 5 # Example maximum generations without improvement\n",
    "\n",
    "# Initialize variables to track convergence\n",
    "best_fitness_value = None\n",
    "generations_without_improvement = 0\n",
    "\n",
    "# Run the genetic algorithm\n",
    "NGEN =  50  # Number of generations\n",
    "CXPB =  0.7  # Crossover probability\n",
    "MUTPB =  0.2  # Mutation probability\n",
    "\n",
    "for gen in tqdm(range(NGEN)):\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "\n",
    "    # Select the next generation individuals\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    # Clone the selected individuals\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    # Apply crossover and mutation on the offspring\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < CXPB:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        if random.random() < MUTPB:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in offspring if ind.fitness.values == (0,)]\n",
    "    # print(invalid_ind)\n",
    "    invalid_ind.extend([ind for ind in offspring if not ind.fitness.valid])\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        # print(type(ind.fitness.values[0]), ind.fitness.values, type(fit), fit)\n",
    "        ind.fitness.values = fit\n",
    "    # Update the statistics\n",
    "    record = stats.compile(offspring)\n",
    "    # print(record, offspring[0].fitness.values)\n",
    "    logbook.record(gen=gen, **record)\n",
    "\n",
    "    # Check for convergence\n",
    "    current_best_fitness = record[\"average\"] \n",
    "    if (best_fitness_value is None) or (abs(current_best_fitness - best_fitness_value) / best_fitness_value < convergence_threshold):\n",
    "        best_fitness_value = current_best_fitness\n",
    "        generations_without_improvement = 0\n",
    "    else:\n",
    "        generations_without_improvement += 1\n",
    "\n",
    "    # Termination condition\n",
    "    if generations_without_improvement >= max_generations_without_improvement:\n",
    "        print(f\"Converged after {gen} generations\")\n",
    "        break\n",
    "\n",
    "    # Replace population with the offspring\n",
    "    population[:] = offspring\n",
    "\n",
    "# Extract the best individual\n",
    "best_ind = tools.selBest(population,  1)[0]\n",
    "best_score = best_ind.fitness.values[0]\n",
    "\n",
    "# save the logbook to a dataframe\n",
    "logbook_df = pl.DataFrame(logbook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save logbook_df to a csv file\n",
    "logbook_df.to_csv('logbook_df.csv')\n",
    "# save eval_df to a csv file\n",
    "eval_df.to_csv('eval_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0009400000000000012, 4.93980307589126, 4] 0.9054065712093887\n"
     ]
    }
   ],
   "source": [
    "print(best_ind, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comparison, best_take_profit, best_stop_loss, best_threshold = best_ind #, best_lot_size,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_2023 = making_calculations(df_2023, initial_brick_size, best_inc, best_max, best_ma, best_signal, initial_brick_size*2, initial_lot_size)\n",
    "# results_2024 = making_calculations(df_2024, initial_brick_size, best_inc, best_max, best_ma, best_signal, initial_brick_size*2, initial_lot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_brick</th>\n",
       "      <th>best_sma_length</th>\n",
       "      <th>best_smoothing_sma_length</th>\n",
       "      <th>profit</th>\n",
       "      <th>days_in_drawdown</th>\n",
       "      <th>profit_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00094</td>\n",
       "      <td>4.939803</td>\n",
       "      <td>4</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>117353.167227</td>\n",
       "      <td>119.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00094</td>\n",
       "      <td>4.939803</td>\n",
       "      <td>4</td>\n",
       "      <td>117353.167227</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.707879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_brick   best_sma_length   best_smoothing_sma_length         profit  \\\n",
       "0     0.00094          4.939803                           4       3.000000   \n",
       "1     0.00094          4.939803                           4  117353.167227   \n",
       "\n",
       "   days_in_drawdown  profit_factor  \n",
       "0     117353.167227     119.000000  \n",
       "1        119.000000       0.707879  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_parameters = pd.read_excel('C:/Users/WilliamFetzner/Documents/Trading/best_parameters_dsma.xlsx')\n",
    "# bp_colnames = best_parameters.columns\n",
    "# results_3mo = making_calculations(best_brick, int(best_sma_length), int(best_smoothing_sma_length), initial_lot_size)\n",
    "# new_row_added = pd.concat([best_parameters, pd.DataFrame({bp_colnames[0]: [best_brick], bp_colnames[1]: [best_sma_length], bp_colnames[2]: [best_smoothing_sma_length], \n",
    "#            bp_colnames[3]: results_3mo[0], bp_colnames[4]: results_3mo[1], bp_colnames[5]: results_3mo[2]})], ignore_index=True)\n",
    "# # save the best_parameters dataframe to the best_parameters_IP.xlsx file\n",
    "# new_row_added.to_excel('C:/Users/WilliamFetzner/Documents/Trading/best_parameters_dsma.xlsx', index=False)\n",
    "# new_row_added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

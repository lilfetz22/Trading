# Variables to adjust:

# type "newday"

## IMPULSE MACD and PSAR
1. Renko Brick Size
2. 15 min chart or Renko chart
3. Renko and just PSAR? or Renko, PSAR, and Impulse? 
4. When to exit? 
    a. Vu's strategy (one renko brick in same direction, take half, set SL to 0, then exit on Renko bar reversal)
    b. one renko brick in same direction, take half, set SL to 0, then exit at n fib levels above entry (optimize n)
5. When to Enter?
    a. PSAR and Impulse
    b. PSAR only
    c. Impulse only
5. Optimize inputs into PSAR and Impulse
6. What would happen if I didn't take half after one renko brick in same direction? 
    

## Things to investigate
1. Weekend holds - DONE
2. Max daily drawdown = -$4,500 -> start Nova at 5 lot size, Msolutions is good for 10 lot size
3. Swap rates - DONE
4. Rejoin with original data to see if high's and low's are correct - DONE
5. find out how often the input data skips minutes - 0.5% of the time, there are 98 instances of 121 min gaps, then several 60 min gaps, 1814 gaps in total that are greater than 1 min - DONE
6. Create an optimization function for each of the parameters
    a. Check how well the hours for turning off IP extend to new data
    b. calculate win rate - DONE
    c. how often do the parameters need to be refreshed? 
        - say I optimized the parameters on the last 4 months of data, then applied them to the next week, did it perform significantly better than just using the full year's parameters? 
    d. make sure the optimization function negatively imposes constraints for any value outside of the original bounds of each list and will ask for a new sample if the value is impossible and will throw an error in the making_calculations function. - DONE
7. figure out how to add in the Max payout to the notebook
8. integrate news events to not take any trades in the bot 5 minutes before and after the news event - DONE




## Next Steps:
1. See if I can connect to MetaTrader 5 and run this strategy on a demo account - NOT POSSIBLE
      - after I get the impulse_psar working on MT4, then I can only choose brokers who offer MT5 (Nova) and transition over in the future
2. If that doesn't work, then see if I can translate this strategy to create a Expert Advisor in MetaTrader 4

## EA in MetaTrader 4
1. Get Renko bars on the chart and then be able to extract their values
     - determine how far back the indicator stores values that can be accessed by the EA for PSAR and Impulse
2. replicate the PSAR function - DONE found an indicator that does this
3. replicate the Impulse MACD function - DONE found an indicator that does this
4. figure out how to test EA on historical data - DONE
5. Define Entry Conditions - DONE
6. Define Exit Conditions - DONE
7. Define S/L - DONE
8. Define primary and secondary T/P - DONE
9. Add in calculations for the psar and impulse MACD so I'm not depending on the indicators
10. integrate news events to not take any trades in the bot 5 minutes before and after the news event - DONE
11. Make lot size adjustable to where it takes into account the average loss and the balance of the account so that we can make 4 consecutive losses and not go under the Max_drawdown limit
    a. otherwise if max_drawdown is not a problem, then take the average lot size * 2 and use the integer value of that as the lot size - DONE
12. find out the profit for running IP for the entire year
    a. calculate the win_rate per hour - DONE
    b. Use this to adjust the lot_size
13. Create the pseudo take profit with the closing being after 3 bars that are relatively level

## Debugging:
1. there is still an error with some of the tickets opening when others are still open.  - fixed
2. I need to either protect against errors in Impulse MACD when the Impulse was Null and the Signal was 0.0088 because that will cause the EA to open shorts even though it shouldn't - this is fine

# Steps to add the EA to the chart
1. Open EURUSD 1 min chart
2. Drag Renko EA onto the chart
3. Make sure that live trading is enabled
4. Open offline EURUSD 2 chart
5. Uncheck "offline chart" in the properties
6. Drag the Parabolic SAR and Impulse MACD indicators onto the chart
7. Drag Impulse_PSAR EA onto the chart
8. Make sure that live trading is enabled


## Changes to make EA match the backtest
1. change start time from 3 to 4 am and end time to 4 pm - DONE
2. Handling news events (no new trades 1 hr after news event) - DONE
3. making it where the PSAR has to switch before initiating a new trade - DONE


# New Strategy
Shift everything to just straight python. I have a new connector that will allow me to run python code with a EA in MT4: 
https://github.com/TheSnowGuru/PyTrader-python-mt4-mt5-trading-api-connector-drag-n-drop/tree/master
https://www.mql5.com/en/market/product/58254?source=External#description

1. We can then create a strategy and use reinforcement learning with my PSAR/Impulse/SMI strategy and optimize the parameters 
and then be able to run the strategy in MT4
   - This opens up the door to Renko bars again
   - allows me to stay in python and be able to control the backtesting rather than depending on the MT4 strategy tester


1. Check the integrity of the tickdata - DONE
      - the data looks fairly solid, there were a few instances of the tick data missing a few minutes, but it was a low 
      percentage and pretty consistent across the entire dataset, so nothing to really be concerned about
2. See what impact it will have to not enter a position when the bar closes in the opposite direction


[2024-03-23]
Notes:
Ideas:
# Reinforcement Learning
- I should create a reinforcement learning model that will decide based upon the Double SMA, bollinger bands and Awesome Oscillator (I can change these later)
whether it is a good time to buy, sell, or hold. If the signal is given to buy/sell with a current order, that order will close, and then if the signal appears
again with the next bar then the order will be opened.
- I do this with the parameters that I have found successful so far which is 8.1 brick size, 3 for both sma and smoothing sma
- after that I can take the models as inputs into the GA and optimize the model parameters with GA with the score being how well the model performs
# Double SMA optimization
- Determine if pyramiding trades works better than just taking one trade at a time
    - I believe that I can do this by finding out how many bars are in between the trades and if they are in the same direction, then creating a new order;
    however, I need to look at some of the losing trades and see why they lost
- a pyramid order should be defined as (brick_size * sma_length)
- I am going to assume that the pyramid order will be placed at the close of the bar by using the current spread information provided
[c++]
    double spread;
    SymbolInfoDouble(Symbol(), SYMBOL_SPREAD, spread);
    Print("Current Spread: ", spread);
[end]

TODO:
[√] investigate losing trades
[√] test pyramiding orders
[√] incorporate spread into closing price for pyramiding_orders

Activity Log:

[2024-04-08]
Notes:
Ideas:

TODO:
[ ] Hyperparameter Tuning: Carefully tune the hyperparameters of your PPO algorithm, such as the learning rate, discount factor, and entropy coefficient, to find the optimal configuration for your trading environment.
[ ] Splitting the Data Appropriately: Instead of an 80/20 split, consider using a more rigorous approach, such as a rolling window or a walk-forward validation technique. This will ensure that your testing data is truly independent and representative of the market conditions your model will face in production.
Activity Log:

[2024-04-09]
Notes:
From phind:
[python]
hyperparameters = {
    'lr': [1e-3, 1e-4, 1e-5], # Learning rate
    'gamma': [0.9, 0.95, 0.99], # Discount factor
    'ent_coef': [0.01, 0.05, 0.1] # Entropy coefficient
}
def train_and_evaluate(lr, gamma, ent_coef):
    model_ppo = PPO(policy_ppo, env_train, verbose=0, lr=lr, gamma=gamma, ent_coef=ent_coef)
    model_ppo.learn(total_timesteps=total_learning_timesteps_ppo, callback=ProgressBarCallback(100))
    # Evaluate the model and return the performance metric
    # For example, you might calculate the average reward over a test set
    # This is a placeholder for your evaluation logic
    return average_reward
best_hyperparameters = None
best_performance = -np.inf

for lr in hyperparameters['lr']:
    for gamma in hyperparameters['gamma']:
        for ent_coef in hyperparameters['ent_coef']:
            performance = train_and_evaluate(lr, gamma, ent_coef)
            if performance > best_performance:
                best_performance = performance
                best_hyperparameters = (lr, gamma, ent_coef)

print(f"Best hyperparameters: {best_hyperparameters}")

best_lr, best_gamma, best_ent_coef = best_hyperparameters
model_ppo = PPO(policy_ppo, env_train, verbose=0, lr=best_lr, gamma=best_gamma, ent_coef=best_ent_coef)
model_ppo.learn(total_timesteps=total_learning_timesteps_ppo, callback=ProgressBarCallback(100))

# Walk forward validation: 
env_train = MyForexEnv(
    df=FOREX_EURUSD_RENKO[:split],
    window_size=10,
    frame_bound=(10, split),
    trade_fee=0.0001,
    spread=0.0001,
    spread_bool=False,
    unit_side='right',
    sma_length=4,
    smoothing_sma=4
)

env_test = MyForexEnv(
    df=FOREX_EURUSD_RENKO[split:],
    window_size=10,
    frame_bound=(split, len(FOREX_EURUSD_RENKO)),
    trade_fee=0.0001,
    spread=0.0001,
    spread_bool=False,
    unit_side='right',
    sma_length=4,
    smoothing_sma=4
)

# Set the random seed
seed_ppo = 42
torch.manual_seed(seed_ppo)
random.seed(seed_ppo)
np.random.seed(seed_ppo)

# Initialize the PPO model
model_ppo = PPO(policy_ppo, env_train, verbose=0)

# Iterate through the training and testing sets
for i in range(0, len(FOREX_EURUSD_RENKO) - split, 10):
    # Update the training and testing environments
    env_train.frame_bound = (10, split + i)
    env_test.frame_bound = (split + i, len(FOREX_EURUSD_RENKO))

    # Train the model
    model_ppo.learn(total_timesteps=total_learning_timesteps_ppo // 10)

    # Evaluate the model on the testing set
    obs_test, info_test = env_test.reset()
    done_test = False
    total_reward_test = 0
    while not done_test:
        action_test, _ = model_ppo.predict(obs_test)
        obs_test, reward_test, done_test, info_test = env_test.step(action_test)
        total_reward_test += reward_test

    print(f"Iteration {i}: Testing Reward = {total_reward_test}")

# Save the model after training
model_ppo.save("ppo_model.pkl")

# Load the saved model
model_ppo = PPO.load("ppo_model.pkl", env=env_train)

[end]

Ideas:
TODO:
[ ] continue working with Perplexity to implement Hyperparameter Tuning

Activity Log:
- I created the news events for the entire time of the data, before it only went back to 2022-01-01
   

[2024-04-10]
Notes:
Ideas:
TODO:
[ ] figure out how to change the frame_bound
[ ] figure out what .reset is doing for the environment
Activity Log:


[2024-04-19]
Notes:
Ideas:
TODO:
[ ] Start working on AI Agents that will find the most profitable method and test it on the data
Activity Log:
- I figured out how to connect with MT4 to python and run a script! :) 
- currently training the model to find the best hyperparameters

[2024-04-20]
Notes:
- I need to make a hard cut off for the transition from my personal project and work. 
    - 9:45 am is the cut off time for my personal project
    - 9:45 am - 10:00 am is the time for me to transition to work, wrapping up, putting in my commit messages, 
        and prepping for what I need to do in the next session
    - 10-11 am are meetings
    - 11-11:15 am proritize the tasks from Q-C
    - 11:15-5 pm working for CVS
    - 5-6:30 - working for Q-C

Answers to some of the questions below:
- What are the things to consider when taking a model into production?: 
Model Performance: Ensure your model performs well on unseen data and maintains its performance over time. Regularly evaluate your model's performance using appropriate metrics.

Data Pipeline: Your model's performance depends on the quality of data it receives. Ensure your production data pipeline is robust, reliable, and can handle the scale of data your model needs.

Model Versioning: Keep track of different versions of your model and the data they were trained on. This helps in debugging and allows for rollbacks if necessary

Scalability: Your model should be able to handle increasing amounts of data and requests as your user base grows.

Monitoring: Implement monitoring to track your model's performance and health over time. This helps in identifying and addressing issues promptly.

Security and Privacy: Ensure your model complies with all relevant data privacy laws and regulations. Implement necessary security measures to protect your model and data.

Maintenance: Models may need to be retrained over time as data and requirements change. Plan for regular model updates and maintenance.

Infrastructure: Consider the infrastructure needed to support your model in production. This includes server capacity, storage, and networking capabilities.

Integration: Ensure your model can be integrated smoothly with other parts of your system.

Testing: Implement thorough testing to catch and fix any issues before your model goes live. This includes unit tests, integration tests, and end-to-end tests.

Documentation: Document your model, its inputs and outputs, how it should be used, and any known issues or limitations. This is crucial for effective use and maintenance of your model in production.

What are things that most people don't consider when taking a reinforcement learning model into production:
Non-Stationarity: Unlike supervised learning models, RL models interact with an environment that can change over time. This non-stationarity can lead to model degradation if not properly managed.

Exploration vs Exploitation: RL models need to balance exploration (trying new actions to improve) and exploitation (using known information to make the best decision). This balance can be challenging to maintain in a production environment.

Safety: RL models can make harmful decisions during exploration. It's important to implement safety measures to prevent these actions, especially in sensitive environments.

Reward Engineering: Designing a good reward function can be difficult but it's crucial for RL models. A poorly designed reward function can lead to unintended behavior.

Sample Efficiency: RL models often require a lot of data (experiences) to learn effectively. In a production environment, collecting this data can be time-consuming and costly.

Model Interpretability: RL models can be hard to interpret. This can make it difficult to understand why the model is making certain decisions, which is problematic in many production environments.

Real-time Learning: Many RL applications require the model to learn and make decisions in real time. This can be challenging from a computational perspective.

Evaluation: Evaluating RL models can be tricky as performance can be highly variable and dependent on the initial conditions of the environment.

Reproducibility: RL models can be hard to reproduce due to their interaction with the environment and the stochastic nature of many RL algorithms.

Continuous Deployment: Unlike other models, RL models often need to be continuously updated and deployed as they learn from new data. This requires a robust infrastructure that can support continuous deployment and monitoring.
Ideas:
Questions to answer:
- How often do I need to update the hyperparameters?
- will the model be able to retrain in between receiving new data?
- How well does the model perform on a new week of data?
- What are the things to consider when taking a model into production? -Above-
- What are things that most people don't consider when taking a reinforcement learning model into production?-Above-
- How much data do I need to train on? 
- How do I implement combining this model with my current account protection strategies?
TODO:
[ ] Start working on AI Agents that will find the most profitable method and test it on the data
[ ] investigate the ability to use TimeGPT
[ ] create the logic to implement the model in production while the model is still training and finding the hyperparameters
[ ] combine this model with my current account protection strategies?
      - current account protection safeguards:
            1. controlling lot size
                a. trades in runway
                   - I did this by finding the average lot size, multiplying it by 2 for the max lot size
                   - then I looked at both the daily drawdown max and the total drawdown max compared to where I currently stand in drawdown. Then whichever is the least, I divided that number by the number of trades I wanted in the runway. 
                   - then I found the minimum of that and the max lot size to determine the proposed lot size 
                
            2. start and end hours
            3. max_daily_drawdown percent
            4. max total drawdown
            5. payout max
            6. news events
            7. long orders closed at 4-55 pm to protect against swap, shorts kept open
            8. closing any order on Friday at 4 pm 
            9. 
Activity Log:
- answered questions that I have outstanding
- created the logic to be able to next test weekly hyperparameter updates

